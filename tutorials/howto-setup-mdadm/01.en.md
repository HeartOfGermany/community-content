---
SPDX-License-Identifier: MIT
path: "/tutorials/howto-setup-mdadm"
slug: "howto-setup-mdadm"
date: "2019-06-18"
title: "Software RAID under Linux"
short_description: "Installation and configuration of a software RAID (mdadm) on Linux systems."
tags: ["Linux", "RAID", "mdadm"]
author: "Markus"
author_link: "https://github.com/BackInBash"
author_img: "https://avatars3.githubusercontent.com/u/48181660"
author_description: ""
language: "en"
available_languages: ["de", "en"]
header_img: "header-8"
cta: "dedicated"
---

## Introduction

This tutorial is about the installation, setup and administration of a `mdadm` software RAID on Linux systems.

<br>

**Preconditions**

+ An installed Linux OS
+ A dedicated server with at least 2 free partitions on 2 separate drives
+ SSH Root access

<br>

**Some explenations**

- We will refer to /dev/md/0 in this tutorial, which makes the use of /dev/md/0 and /dev/md0 possible in most tools. You can substitute this simply with /dev/md0, but this will make the use of /dev/md/0 impossible, if you have any scripts or programs using this. If you already created RAIDs and named them /dev/md0, you have to continue using this instead of /dev/md/0
- You will have to replace /dev/md/0 with your RAIDs name, it could be /dev/md127 for example.
- We will also use the following drives: /dev/sdb - /dev/sde >>> You have to replace them with the drives you are using
- We will use the following partitions: /dev/sdb1 - /dev/sde1 >>> You have to replace them with the partitions you are using
- Even if you only need one step, please read all in context, or critical information might get lost
- We will refer to /etc/mdadm/mdadm.conf, but on some systems this config might be located at /etc/mdadm.conf. Please use the correct path in that case.
- There are a lot of parameters for mdadm. It makes no sense to list and explain them all here due to the sheer amount of them. If you need further parameters in addition to the ones used and described here, please refer to the man pages of mdadm. You will find a detailed explenation on all mdadm parameters there.

<br>

## Step 1 - Preparations

First you should think about which RAID system you need to run. This depends on the target and how many drives are installed in the server itself.

>
>***Note:** A RAID should not be seen as a data backup as it does not provide protection against data loss. It only increases the availability of the data and often the performance. Also the capacity and performance is always based around the drive with the worst values each.*
>

<br>

### Step 1.1 - RAID level selection

Choosing the right RAID level is not easy and depends on several factors:
+ How many drives does the server have?
+ What are your goals?
    + More storage space / Less availability
    + Higher availability / Less storage space
    + A compromise of both

**Here is a list of the most used RAID levels:**

<br>

**RAID0**:

If there is a group of two or more partitions, the partitions are logically combined to one partition.
Here the availability is reduced. If one of the drives is defective **automatically all data is lost**.

The capacity is: Drive_Count*DriveCapacity

[Learn more about RAID0](https://en.wikipedia.org/wiki/RAID#Standard_levels)
+ Pro
    + Adds all available storage
    + Increases drive performance
+ Contra
    + In case of a drive failure the data of all drives are inevitable lost
    + RAID 0 can introduce instability

<br>

**RAID1**:

If there is a group of two or more drives, the data is mirrored on each partition.

The capacity is: 1*DriveCapacity

[Learn more about RAID1](https://en.wikipedia.org/wiki/RAID#Standard_levels)
+ Pro
    + Increases the reliability / availability of the data.
    + Increases the reading speed of the data.
    + Fastest RAID with 1 RAID layer (no parity calculation)
    + No additional CPU load
+ Contra
    + The available storage space is halved.
    + Writing speed unaffected or worse if one drive performs worse

<br>

**RAID5**:

Is a group of three or more drives, where the capacity of one drive is used for parity data (redundancy).

The capacity is: (Drive_Count - 1)*DriveCapacity

[Learn more about RAID5](https://en.wikipedia.org/wiki/RAID#Standard_levels)
+ Pro
    + Increased reliability / availability of data
    + Optimal storage utilization
    + Increases the reading speed of the data
+ Contra
    + Less performance with write accesses
    + Higher CPU usage

Examples:

- 3 drives á 1TB = 2TB (33% capacity loss)
- 4 drives á 1TB = 3TB (25% capacity loss)
- 5 drives á 1TB = 4TB (20% capacity loss)

<br>

**RAID6**:

Is a group of four or more drives, where the capacity of one drive is used for parity data (redundancy).

The capacity is: (Drive_Count - 2)*DriveCapacity

[Learn more about RAID6](https://en.wikipedia.org/wiki/RAID#Standard_levels)
+ Pro
    + Further increased reliability / availability of data
    + Optimal storage utilization
    + Increases the reading speed of the data
+ Contra
    + Less performance with write accesses
    + Even higher CPU usage

Examples:

- 4 drives á 1TB = 2TB (50% capacity loss)
- 5 drives á 1TB = 3TB (40% capacity loss)
- 6 drives á 1TB = 4TB (33% capacity loss)

<br>

**Hybrid RAID / Multi-Layer Raid**
The above RAID levels are often combined with the same logic as:
- RAID 1+0 - (Extreme performance+Redundancy)
- RAID 0+1 - (Extreme performance+Redundancy)
- RAID 5/6+0 - (High redundancy+Performance)
- RAID 0+5/6 - (High redundancy+Performance)
- RAID 5/6+1 - (Very High redundancy)
- RAID 1+5/6 - (Very High redundancy)

<br>

### Step 1.2 - Preparing RAID setup: List of drives in the system

For a short and clear list of all available block devices, the command `lsblk` can be used.
Here is a sample output on a cloud instance with 2 attached drives:
```console
root@raid-test-server:~# lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda       8:0    0 19.1G  0 disk 
├─sda1    8:1    0   19G  0 part /
├─sda14   8:14   0    1M  0 part 
└─sda15   8:15   0  122M  0 part /boot/efi
sdb       8:16   0   10G  0 disk 
sdc       8:32   0   10G  0 disk 
```

We will contentrate on `sdb` and `sdc`. You can also see, that both drives have no mountpoint and no partition at the moment.

For a list with more detailed information, `fdisk -l` can be used.
Here is a sample output:
```console
root@raid-test-server:~# fdisk -l


Disk /dev/sdc: 10 GiB, 10737418240 bytes, 20971520 sectors
Disk model: Volume          
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes


Disk /dev/sda: 19.07 GiB, 20480786432 bytes, 40001536 sectors
Disk model: QEMU HARDDISK   
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: gpt
Disk identifier: 1B082DDE-A26B-46D7-9223-F1BABA7BBD2D

Device      Start      End  Sectors  Size Type
/dev/sda1  253952 40001502 39747551   19G Linux filesystem
/dev/sda14   2048     4095     2048    1M BIOS boot
/dev/sda15   4096   253951   249856  122M EFI System

Partition table entries are not in disk order.


Disk /dev/sdb: 10 GiB, 10737418240 bytes, 20971520 sectors
Disk model: Volume          
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
```

>
>***Note**:
>For a software RAID, it is not necessary to add the entire drive to the RAID. Single partitions are sufficient.*
>

<br>

## Step 2 - Create a software RAID

### Step 2.1 - Preparing the drives

First the drives must be formatted accordingly.

You can download parted the following way, if the upcomming commands fail due to parted not being found:

`apt update && apt install parted -y`

Create a new, empty partition table on the drive:

+ Typically you would use UEFI:
  ```bash


  parted /dev/sdc mklabel gpt
  ```

+ If you need a bootable legacy/BIOS partition you can use:
  ```bash
  parted /dev/sdb mklabel msdos
  parted /dev/sdc mklabel msdos
  ```

>
>***Note** gpt/UEFI partition scheme is required for partitions beyond the 2TB size.*
>

Create a partition on the drive:
```bash
parted -a optimal -- /dev/sdb mkpart primary 0% 100%
parted -a optimal -- /dev/sdc mkpart primary 0% 100%
```




If you are running an old system you might need to mark the newly created partition as a RAID partition:
```bash
parted /dev/sdb set 1 raid on
parted /dev/sdc set 1 raid on
```

<br>

### Step 2.2 - Create the software RAID

Under Linux, `mdadm` is the main tool. It is the interface to the RAID functions of the kernel.

To create a RAID 1 the following command is sufficient:

```bash
mdadm --create /dev/md/0 --auto md --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1
```

If we pretend we would have a 3rd Drive to add, we can create a RAID 5. Please also follow Step 2.1 for the 3rd drive and any other drives you might want to add in this case. If we pretend that the 3rd partition was /dev/sdd1 The following command is sufficient to create a RAID 5:

```bash
mdadm --create /dev/md/0 --auto md --level=5 --raid-devices=3 /dev/sdb1 /dev/sdc1 /dev/sdd1
```

If we pretend we would have a also a 4th Drive to add, we can create a RAID 6. The following command is sufficient to create it:

```bash
mdadm --create /dev/md/0 --auto md --level=5 --raid-devices=3 /dev/sdb1 /dev/sdc1 /dev/sdd1 /dev/sde1
```

<br>

The newly created block device `mdX` can be used immediately and the system can also be shut down or restarted during this time. During a Sync or rebuilt the performance might be noticably reduced.
The current status of the RAID creation can be queried with the following command:

```bash
watch cat /proc/mdstat
```

During the creation it would show:

```console
Personalities : [raid0] [raid1]
mdX : active raid1 sdb1[1] sdc1[0]
      8384448 blocks super 1.2 [2/2] [UU]
      [==============>.......] check = 74.7% (6267520/8384448) finish=0.1min speed=202178K/sec
```

We need to Format the newly created RAID and in this case will use ext4, which is good for most cases:

```bash
mkfs.ext4 /dev/md/0
```

To use the RAID in the system we need to create a directory and than mount the RAID to it. In this case we name the folder /mnt/md0-mount and mount /dev/md/0 to it.

```bash
mkdir /mnt/md0-mount
mount /dev/md/0 /mnt/md0-mount
```

<br>

**Persistent RAID re-creation**

You want your RAID to automatically assemble (correctly) on each reboot, so run the following 2 commands:

```bash
mdadm --detail --scan >> /etc/mdadm/mdadm.conf
update-initramfs -u
```
Running the first command without writing it to mdadm.conf will show us the information, mdadm requires to start the RAID properly on boot up:

```bash
root@raid-test-server:~# mdadm --detail --scan
ARRAY /dev/md/0 metadata=1.2 name=raid-test-server:1 UUID=e2d1049e:d6c61312:5a8d0975:3fac53c0
```

If you reboot your server and mdadm has no config for your RAID it will name it differently, if it assembles it:

```
root@raid-test-server:~# lsblk
NAME      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sda         8:0    0 19.1G  0 disk  
├─sda1      8:1    0   19G  0 part  /
├─sda14     8:14   0    1M  0 part  
└─sda15     8:15   0  122M  0 part  /boot/efi
sdb         8:16   0   10G  0 disk  
└─sdb1      8:17   0   10G  0 part  
  └─md127   9:127  0   10G  0 raid1 
sdc         8:32   0   10G  0 disk  
└─sdc1      8:33   0   10G  0 part  
  └─md127   9:127  0   10G  0 raid1 
```

<br>

**Persistent RAID mount**

After a reboot the mount will be gone. To make it persistent and mount the drive again on reboot we need to do the following:
- Get UUID of the RAID for a reliable reference (would also work, if md0 gets renamed to md1)
- Write a new entry for it in /etc/fstab

```bash
root@raid-test-server:~# blkid | grep md0
/dev/md0: UUID="2e641692-d8c3-4b4c-ad08-923af60b6e49" BLOCK_SIZE="4096" TYPE="ext4"
```
The UUID is: 2e641692-d8c3-4b4c-ad08-923af60b6e49

Please replace the UUID **and only the UUID** of the first following command with your own UUID. Than run the 2 commands as listed here:


```bash
root@raid-test-server:~#       echo "UUID=2e641692-d8c3-4b4c-ad08-923af60b6e49 /mnt/md0-mount ext4 defaults 0 2" >> /etc/fstab
root@raid-test-server:~#       echo $'\n\033[0;32m     Entries in your /etc/fstab: \033[0;34m';cat /etc/fstab | grep . | egrep -v '#.*';echo $'\n\033[0m'

     Entries in your /etc/fstab: 
UUID=c8d00ef8-b9d3-4c55-8007-3dec27a2891a /               ext4    errors=remount-ro 0       1
UUID=3612-8A38  /boot/efi       vfat    umask=0077      0       1
/dev/sr0        /media/cdrom0   udf,iso9660 user,noauto     0       0
UUID=2e641692-d8c3-4b4c-ad08-923af60b6e49 /mnt/md0-mount ext4 defaults 0 2


```

The last line should consist of **UUID=your_uuid /mount/folder xt4 defaults 0 2** as clearly visible in our example. Only if it does, reboot and test if the mount works by looking at the output of the command `lsblk`. If not, use nano or your preffered editor to remove the line and double check your UUID and command for any spelling errors.

If you made any mistakes and your server does not boot anymore, you can simply reboot the server into the rescue system and do the following steps:

```bash
mount /dev/sda1 /mnt
nano /mnt/etc/fstab
```

Pretending your OS is located on /dev/sda1 you can now edit fstab and remove the last line. After saving with `<CTRL>+S` and exiting with `<CTRL>+X` you can reboot your server again. Repeat the steps as needed.

<br>

### Step 2.3 - Adding a Hot-Spare device

If you want to further harden your RAID and automatically rebuild a degraded RAID if a drive fails, you can use a Hot-Spare drive. Once your RAID is in a degraded state, the Hot-Spare drive will automatically get attached to the RAID and mdadm rebuilds your RAID.

In this example we pretend, that /dev/sdd is our 3rd drive for our RAID 1, which requires 2 Drives. We also already performed all steps from Section 2.1 on it.

```bash
mdadm --manage /dev/md/0 --add-spare /dev/sdd1
```

You can see the status with the following command:

```bash
root@raid-test-server:~# mdadm --detail /dev/md/0 | egrep '(Devices|State|Number|dev|Size)'
/dev/md/0:
        Array Size : 10474496 (9.99 GiB 10.73 GB)
     Used Dev Size : 10474496 (9.99 GiB 10.73 GB)
      Raid Devices : 2
     Total Devices : 3
             State : clean 
    Active Devices : 2
   Working Devices : 3
    Failed Devices : 0
     Spare Devices : 1
    Number   Major   Minor   RaidDevice State
       0       8       33        0      active sync   /dev/sdc1
       1       8       17        1      active sync   /dev/sdb1
       2       8       49        -      spare   /dev/sdd1

```

<br>

**Testing the Hot-Spare**

In this case we just unmounted /dev/sdb from our cloud instance using the web interface and than quickly checked the results:

```bash
root@raid-test-server:~# cat /proc/mdstat
Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sdd1[2](S) sdb1[1] sdc1[0]
      10474496 blocks super 1.2 [2/2] [UU]
      
unused devices: <none>

root@raid-test-server:~# lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 19.1G  0 disk  
├─sda1    8:1    0   19G  0 part  /
├─sda14   8:14   0    1M  0 part  
└─sda15   8:15   0  122M  0 part  /boot/efi
sdc       8:32   0   10G  0 disk  
└─sdc1    8:33   0   10G  0 part  
  └─md0   9:0    0   10G  0 raid1 /mnt/md0-mount
sdd       8:48   0   10G  0 disk  
└─sdd1    8:49   0   10G  0 part  
  └─md0   9:0    0   10G  0 raid1 /mnt/md0-mount
sr0      11:0    1 1024M  0 rom   

root@raid-test-server:~# cat /proc/mdstat
Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sdd1[2] sdc1[0]
      10474496 blocks super 1.2 [2/1] [U_]
      [==>..................]  recovery = 11.7% (1229632/10474496) finish=1.1min speed=136625K/sec
      
unused devices: <none>

#### Waiting a few seconds till the 10GB RAID is rebuilt ####

root@raid-test-server:~# cat /proc/mdstat
Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sdd1[2] sdc1[0]
      10474496 blocks super 1.2 [2/2] [UU]
```

At first all 3 drives are visible, but only a second later lsblk already shows /dev/sdb as not-existent. Viewing cat/proc/mdstat showed, that the RAID is already rebuilding. This automation can save time, since a degraded array is very risky to operate. It also can be harmfull however. If the stage of the drives does not get monitored properly and another drive is on the way out, syncing a whole drive could cause so much wear, that the RAID might break during a resinc. If only a few kilobytes of data are really important and the rest can be rebuilt quickly, than it would propably be better to backup these few files before rebuilding the RAID. This limits the access to the degraded arrray and prevents further failure. This of course only works, if you actively monitor your RAID and do not wonder, why half a year later your webservice is no longer accessible.

We also attached /dev/sdb again to the server. However it is not automatically adding itself to the RAID as a spare now. To achive this goal, you need to define a new spare:

```bash
root@raid-test-server:~# mdadm --manage /dev/md/0 --add-spare /dev/sdb1
mdadm: added /dev/sdb1

root@raid-test-server:~# cat /proc/mdstat
Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sdb1[3](S) sdd1[2] sdc1[0]
      10474496 blocks super 1.2 [2/2] [UU]
      
unused devices: <none>

```

The next step was to test the feature again, when /dev/sdd was removed, this time in the official way using the fail and remove feature.

```bash
root@raid-test-server:~# mdadm --manage /dev/md/0 --fail /dev/sdd1
mdadm: set /dev/sdd1 faulty in /dev/md/0

root@raid-test-server:~# mdadm --manage /dev/md/0 --remove /dev/sdd1
mdadm: hot removed /dev/sdd1 from /dev/md/0

root@raid-test-server:~# cat /proc/mdstat 
Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sdb1[2] sdc1[0]
      10474496 blocks super 1.2 [2/1] [U_]
      [=>...................]  recovery =  8.4% (888320/10474496) finish=0.8min speed=177664K/sec
      
unused devices: <none>

```

<br>

## Step 3 - Removing a software RAID

To remove a software RAID, in our case our RAID 1 without the spare, the following steps must be performed:

1. Stop the RAID:
   ```bash
   umount /dev/md/0
   mdadm --stop /dev/md/0
   ```
1. Remove automatic mount entries (`/etc/fstab`)
   ```bash
   nano /etc/fstab
   
   # Now remove the entry from earlyer:
   # UUID=2e641692-d8c3-4b4c-ad08-923af60b6e49 /mnt/md0-mount ext4 defaults 0 2
   # Update initramfs by running the following command
   
   update-initramfs -u
   ```
1. Delete RAID entries in `mdadm.conf`.
   `ARRAY /dev/md/0 metadata=1.2 name=raid-test-server:1 UUID=e2d1049e:d6c61312:5a8d0975:3fac53c0`
1. Delete the superblock of the used partitions:
   ```bash
   mdadm --zero-superblock /dev/sdb1 /dev/sdc1
   ```
1. Disable RAID flag:
   ```bash
   parted /dev/sdb set 1 raid off
   parted /dev/sdc set 1 raid off
   ```

<br>

## Step 4 - Managing a software RAID

### Step 4.1 - Get RAID status

A short list of all RAIDs in the system can be obtained with the output of the file `/proc/mdstat` using cat.
```console
root@raid-test-server:~# cat /proc/mdstat
Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sdb1[3](S) sdd1[2] sdc1[0]
      10474496 blocks super 1.2 [2/2] [UU]

md2 : active raid1 sdb3[1] sdc3[0]
      536739840 blocks super 1.2 [2/2] [UU]
      bitmap: 3/4 pages [12KB], 65536KB chunk

md1 : active raid1 sdb2[1] sdc2[0]
      1047552 blocks super 1.2 [2/2] [UU]

unused devices: <none>
```

A more exact output on a specific RAID is done with the command `mdadm --detail /dev/md/0`. However if we only want a few informations, we use grep to only show what we want.
```bash
root@raid-test-server:~# mdadm --detail /dev/md/0 | egrep '(Devices|State|Number|dev|Size)'
```

Here is an example output:
```console
/dev/md/0:
        Array Size : 10474496 (9.99 GiB 10.73 GB)
     Used Dev Size : 10474496 (9.99 GiB 10.73 GB)
      Raid Devices : 2
     Total Devices : 3
             State : clean 
    Active Devices : 2
   Working Devices : 3
    Failed Devices : 0
     Spare Devices : 1
    Number   Major   Minor   RaidDevice State
       0       8       33        0      active sync   /dev/sdc1
       1       8       17        1      active sync   /dev/sdb1
       2       8       49        -      spare   /dev/sdd1
```

<br>

### Step 4.2 - Replace defective drive

To do this, the defective drive must be set to failed and than can be removed from the RAID:
```bash
root@raid-test-server:~# mdadm --manage /dev/md/0 --fail /dev/sdb1
mdadm: set /dev/sdb1 faulty in /dev/md/0

root@raid-test-server:~# mdadm --manage /dev/md/0 --remove /dev/sdb1
mdadm: hot removed /dev/sdb1 from /dev/md/0
```

Never replace a HDD without removing it from the RAID, if possible. This avoids issues further down the line.

Whenever a physical drive needs to be replaced, the new drive must be partitioned. It is important that the new drive has the same partitioning as the defective drive and also matches or exceeds the size of your smallest drive till that moment. For RAID 6 you can in theory use a combination of 1x 512GB, 2x 6TB and 1x 12TB drives. Your Capacity would be limited to:

```
lowest_capacity * (drive_count - 2)
= 512GB * (4 -2) = 1TB
```

>
>**Tip**
>*Based on that for a replacement you would need a drive at least 512GB in size. Talk to your support if you notice, that the capacity of the replacement is lower upon replacement. In the section 5 you will see tips to avoid that issue in generall. There are for example SATA SSDs of sizes 480, 500 and 512 GB. If your first drive setup is 512GB, it might be worth it for you, to lower the partition size to 500GB or slighly less, if you want to be prepared for a replacement with 500GB drives. From an economic standpoint you might not want to lower your RAID capacity even further to 480GB just to be prepared for such a replacement, as 480GB is typically reserved for Enterprise SSDs only anyways. In datacenter applications you will in general always get replacements of the same size or higher, so you should only be prepared for a few hundred megabytes less upon creating a RAID. Even "identical" storage devices can vary in size by a few Bytes (!Not much but it can be critical!)*
>

If your newly added drive does not get automatically detected, you can also run the command `partprobe`. After this you should be able to see it with `lsblk`. 

We need to partition the new drive, it is sufficient to copy the partition table from an existing, functional drive. In our example /dev/sdc is still in the RAID and /dev/sdb was dropped and got replaced and is now waiting to become member of the RAID again.

```bash
# Copying from /dev/sdc because it is healthy
sfdisk -d /dev/sdc > part_table

Restore keeping the same disk & partition IDs**:

# Pasting into /dev/sdb, which was replaced
sfdisk /dev/sdb < part_table

# Direct copy from /dev/sdc to /dev/sdd in this case
sfdisk -d /dev/sdc | sfdisk /dev/sdd
```

If the new drive is partitioned correctly, it can be added to the RAID system again (mdadm internally runs --re-add first to check, if it was part of the RAID before):
```bash
mdadm --manage /dev/md/0 --add /dev/sdd
```

If the new drive is intended as a hotspare, even if it was part of the RAID before, it can be added to the RAID system in that way:
```bash
mdadm --manage /dev/md/0 --add-spare /dev/sdd
```
A drive added as a spare will instantly be used to rebuilt the RAID, if there are members missing.

The progress can be monitored again with the command `watch -n 0.5 cat /proc/mdstat`.

>
>***Note**: If the system is on the RAID itself, it is necessary to install the bootloader on the appropriate drive. Most likely you will not be able to perform this action on the running system. It is best to boot into the rescue system for that procedure. This will avoid any access permission errors from the running system. This is in general done with the following command:*
>```bash
>update grub && grub-install /dev/sda
>```
>

<br>

### Step 4.3 - Expand RAID = Resize RAID

Only RAIDs with levels 1, 4, 5 and 6 can be expanded.

<br>

**RAID 1 > RAID 5**

In our example we have the RAID 1 from before with 2 members and 1 Hot-Spare added to it.
```bash
mdadm /dev/md/0 --add /dev/sdd1
```

Now the RAID can be extended with the new drive:
```bash
root@raid-test-server:~# mdadm --grow --raid-devices=3 --level=5 /dev/md/0 --backup-file=/root/md0.bak

root@raid-test-server:~# cat /proc/mdstat 
Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid5 sdb1[2] sdd1[4] sdc1[3]
      10474496 blocks super 1.2 level 5, 64k chunk, algorithm 2 [3/3] [UUU]
      [==>..................]  reshape = 10.5% (1102296/10474496) finish=1.2min speed=122477K/sec
      
unused devices: <none>

root@raid-test-server:~# mdadm --detail /dev/md/0 | grep Size
        Array Size : 20948992 (19.98 GiB 21.45 GB)
     Used Dev Size : 10474496 (9.99 GiB 10.73 GB)
        Chunk Size : 64K

```

>***Note**:
>In the file specified by `--backup-file` critical areas are saved (typically a few MiB). If the system crashes during the extension, the extension can be continued later using the following command:*
>```bash
>mdadm /dev/md/0 --continue --backup-file=/tmp/md0.bak
>```
>*The backup file **must not** be located on the RAID to be extended! The use of `backup-file` is not mandatory, but strongly recommended.*

<br>

**RAID 5 > RAID 1**

If you have 3 Devices in your RAID 5 it can be possible to go back to RAID 1, if you still have the 2nd halve of your RAIDs space left free. It is tricky and dangerous.

>
>***Note:** You would also have to shrink your filesystem, if it is already so large. This is a complicated procedure, where possibly files or fragments of files need to be written to other sectors (first half of your RAID). In our case we did not extend the filesystem, so we do not need to shrink it and our data will be unaffected.*
>

```bash
root@raid-test-server:~# mdadm --grow --raid-devices=2 --level=1 /dev/md/0
mdadm: Can only convert a 2-device array to RAID1

root@raid-test-server:~# mdadm --grow --raid-devices=2 --level=5 /dev/md/0
mdadm: this change will reduce the size of the array.
       use --grow --array-size first to truncate array.
       e.g. mdadm --grow /dev/md/0 --array-size 10474496

root@raid-test-server:~# mdadm --grow /dev/md/0 --array-size 10474496

root@raid-test-server:~# mdadm --grow --raid-devices=2 /dev/md/0 --backup-file=/root/md0.bak
mdadm: Need to backup 128K of critical section..

root@raid-test-server:~# cat /proc/mdstat
Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid5 sdb1[2] sdd1[4] sdc1[3]
      10474496 blocks super 1.2 level 5, 64k chunk, algorithm 2 [2/2] [UU]
      [====>................]  reshape = 22.3% (2337732/10474496) finish=1.0min speed=129874K/sec
      
unused devices: <none>

#### Waiting a few seconds till the 10GB drive RAID is reshaped ####

root@raid-test-server:~# mdadm --grow --raid-devices=2 --level=1 /dev/md/0
mdadm: level of /dev/md/0 changed to raid1

root@raid-test-server:~# mdadm --detail /dev/md/0 | egrep '(Devices|State|Number|dev|Size)'
/dev/md/0:
        Array Size : 10474496 (9.99 GiB 10.73 GB)
     Used Dev Size : 10474496 (9.99 GiB 10.73 GB)
      Raid Devices : 2
     Total Devices : 3
             State : clean 
    Active Devices : 2
   Working Devices : 3
    Failed Devices : 0
     Spare Devices : 1
    Number   Major   Minor   RaidDevice State
       4       8       49        0      active sync   /dev/sdd1
       3       8       33        1      active sync   /dev/sdc1
       2       8       17        -      spare   /dev/sdb1
```

<br>

**RAID 5 > RAID 5**

If you do not want to change the RAID level, but grow the size of your RAID, this is easily possible with RAID5. In our case we only have 3 drives available, so a RAID 5 with 4 Raid devices would be degraded. In this case we need to --force the action. If you add a spare to the RAID, this is not required.

```bash
root@raid-test-server:~# mdadm --grow --raid-devices=4 --level=5 /dev/md/0 --backup-file=/root/md0.bak
mdadm: Need 1 spare to avoid degraded array, and only have 0.
       Use --force to over-ride this check.

root@raid-test-server:~# mdadm --grow --raid-devices=4 --level=5 /dev/md/0 --backup-file=/root/md0.bak --force
mdadm: Need to backup 384K of critical section..

root@raid-test-server:~# cat /proc/mdstat 
Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid5 sdb1[2] sdd1[4] sdc1[3]
      10474496 blocks super 1.2 level 5, 64k chunk, algorithm 2 [4/3] [UUU_]
      [=>...................]  reshape =  6.1% (644760/10474496) finish=1.2min speed=128952K/sec
      
unused devices: <none>

#### Waiting a few seconds till the 10GB drive RAID is reshaped ####

root@raid-test-server:~# mdadm --detail /dev/md/0 | egrep '(Devices|State|Number|dev|Size)'
/dev/md/0:
        Array Size : 10474496 (9.99 GiB 10.73 GB)
     Used Dev Size : 10474496 (9.99 GiB 10.73 GB)
      Raid Devices : 4
     Total Devices : 3
             State : clean, degraded 
    Active Devices : 3
   Working Devices : 3
    Failed Devices : 0
     Spare Devices : 0
        Chunk Size : 64K
    Number   Major   Minor   RaidDevice State
       4       8       49        0      active sync   /dev/sdd1
       3       8       33        1      active sync   /dev/sdc1
       2       8       17        2      active sync   /dev/sdb1

root@raid-test-server:~# mdadm --grow /dev/md/0 --array-size max

root@raid-test-server:~# mdadm --detail /dev/md/0 | grep Size
        Array Size : 31423488 (29.97 GiB 32.18 GB)
     Used Dev Size : 10474496 (9.99 GiB 10.73 GB)
        Chunk Size : 64K
```

>
>***Note** since we played around and had to manually set the --array-size on the RAID 5 to RAID 1 conversion, we have to use `mdadm --grow /dev/md/0 --array-size max` to get the maximum capacity again.*
>

The file system must still be extended so that the newly created storage space can be used. The extension takes place with the following commands:
```bash
umount /dev/md/0 /mnt    # Unmount the file system
fsck.ext4 -f /dev/md/0   # Force the check, even if it has recently been checked
resize2fs /dev/md/0      # Extend file system to maximum size
mount /dev/md/0 /mnt     # Mount the file system again
```

This will look as follows:

```bash
root@raid-test-server:~# umount /dev/md/0

root@raid-test-server:~# fsck.ext4 -f /dev/md/0
e2fsck 1.46.2 (28-Feb-2021)
/dev/md/0: recovering journal
Pass 1: Checking inodes, blocks, and sizes
Pass 2: Checking directory structure
Pass 3: Checking directory connectivity
Pass 4: Checking reference counts
Pass 5: Checking group summary information
Free blocks count wrong (2551871, counted=2551615).
Fix<y>? yes
Free inodes count wrong (655349, counted=655348).
Fix<y>? yes

/dev/md/0: ***** FILE SYSTEM WAS MODIFIED *****
/dev/md/0: 12/655360 files (0.0% non-contiguous), 67009/2618624 blocks

root@raid-test-server:~# resize2fs /dev/md/0
resize2fs 1.46.2 (28-Feb-2021)
Resizing the filesystem on /dev/md/0 to 7855872 (4k) blocks.
The filesystem on /dev/md/0 is now 7855872 (4k) blocks long.
        
root@raid-test-server:~# mount /dev/md/0 /mnt/md0-mount/
```
>
>***Note**: During the `fsck.ext4` there where indeed 2 errors found, which likely is the result of multiple size changes. Please consider running a file system check after each change. The errors could be fixed and the file that was written was readable again with no issues, where before the check nano gave an error message. We did not add details on the testfile, since it was just used to validate, if the steps documented here work as intended.*
>

<br>

If your /etc/mdadm.conf differs from our example, you might need to edit it: `nano /etc/mdadm/mdadm.conf` and correct the section with num-devices and level, if it is part of your config like in this example:

OLD:
ARRAY /dev/md/0 level=1 num-devices=2 metadata=1.2 name=raid-test-server:1 UUID=e2d1049e:d6c61312:5a8d0975:3fac53c0

NEW:
ARRAY /dev/md/0 level=5 num-devices=3 metadata=1.2 name=raid-test-server:1 UUID=e2d1049e:d6c61312:5a8d0975:3fac53c0

<br>

In our case our RAID config did not contain any critical data that would need to be adjusted, so we tried a reboot and got:

```bash
root@raid-test-server:~# mdadm --detail /dev/md/0 | egrep '(Devices|State|Number|dev|Size)' && lsblk | egrep '(─sd.1 |md)'
/dev/md/0:
        Array Size : 31423488 (29.97 GiB 32.18 GB)
     Used Dev Size : 10474496 (9.99 GiB 10.73 GB)
      Raid Devices : 4
     Total Devices : 3
             State : clean, degraded 
    Active Devices : 3
   Working Devices : 3
    Failed Devices : 0
     Spare Devices : 0
        Chunk Size : 64K
    Number   Major   Minor   RaidDevice State
       4       8       33        0      active sync   /dev/sdc1
       3       8       17        1      active sync   /dev/sdb1
       2       8       49        2      active sync   /dev/sdd1
├─sda1    8:1    0   19G  0 part  /
└─sdb1    8:17   0   10G  0 part  
  └─md0   9:0    0   30G  0 raid5 /mnt/md0-mount
└─sdc1    8:33   0   10G  0 part  
  └─md0   9:0    0   30G  0 raid5 /mnt/md0-mount
└─sdd1    8:49   0   10G  0 part  
  └─md0   9:0    0   30G  0 raid5 /mnt/md0-mount
```


As you can see, our RAID assembled correctly and still gets properly mounted. 

>
>***Note**: We combined mdadm with egrep and lsblk with grep to only get the informations required)*
>

<br>

### Step 4.4 - RAID monitors

To monitor the RAID, this entry can be stored as a crontab (`crontab -e`):
```bash
0 0 * * * /usr/share/mdadm/checkarray --cron --all --quiet >/dev/null 2>&1 # Runs every day at 00:00 AM
```

If you have a mail server installed on your host or run a configured `msmtp` client, you can let mdadm sent mails about failures to your location. This can be set in /etc/mdadm.conf. You can edit it as follows: `nano /etc/mdadm/mdadm.conf` and change the lines as listed here:

- Situation 1 - Mailserver installed:
  Change the follwing line to match:
  ```bash
  # instruct the monitoring daemon where to send mail alerts
  MAILADDR your_receiver_mail@mail_provider.com
  ```

- Situation 2 - msmtp client installed:
  Change the follwing line to match:
  ```bash
  # instruct the monitoring daemon where to send mail alerts
  MAILADDR your_receiver_mail@mail_provider.com
  MAILFROM your_sender_mail@mail_provider.com
  ```

<br>

## Step 5 - Learning and training on mdadm


#### Situation 1: Training on a degraded RAID 5 and RAID confgig not persistently saved yet

For testing and debugging you can create a RAID 5 with only 2 drives by substituting the 3rd drive with `missing`. This is actually a valid RAID status, since a drive could fail and you would only have 2 drives in the RAID in this case.

```bash
mdadm --create /dev/md/0 --auto md --level=5 --raid-devices=3 /dev/sdb1 /dev/sdc1 missing
```

To see the state and name of (all) the RAID(s):

```bash
root@raid-test-server:~# cat /proc/mdstat
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid5 sdc1[1] sdb1[0]
      20948992 blocks super 1.2 level 5, 512k chunk, algorithm 2 [3/2] [UU_]
```
As you can see, there is a drive missing on RAID md0, indicated by both [3/2] and [UU_]

We can also stop the RAID and see what happens, when we want to start it again:

```bash
root@raid-test-server:~# mdadm --manage /dev/md/0 --stop
mdadm: stopped /dev/md/0
root@raid-test-server:~# mdadm /dev/md/0 --assemble
mdadm: /dev/md/0 not identified in config file.
root@raid-test-server:~# mdadm --assemble --scan
mdadm: /dev/md/0 has been started with 2 drives (out of 3).
```

As you can see, mdadm did not find /dev/md/0 in the config and we had to use the scan function to re-assemble the RAID. It also states, that it started with 2 of 3 drives.

Unfortunately mdadm does not allow us to let one more drive fail to simulate another drive failure:
```bash
root@raid-test-server:~# mdadm --manage /dev/md/0 --fail /dev/sdc1
mdadm: set device faulty failed for /dev/sdc1:  Device or resource busy
```

In the documentation you might read, that you can stop the syncing, but this RAID actually is already in an idle state. But we can forcefully remove it and re-scan it as follows. Follow each command and its output carefully to see, what situation mdadm can handle well and which situation finally kills the RAID once and for all:

```bash
root@raid-test-server:~# echo 1 > /sys/block/sdc/device/delete

root@raid-test-server:~# lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda       8:0    0 19.1G  0 disk 
├─sda1    8:1    0   19G  0 part /
├─sda14   8:14   0    1M  0 part 
└─sda15   8:15   0  122M  0 part /boot/efi
sdb       8:16   0   10G  0 disk 
└─sdb1    8:17   0   10G  0 part 
sr0      11:0    1 1024M  0 rom  

root@raid-test-server:~# cat /proc/mdstat
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] 
unused devices: <none>

root@raid-test-server:~# mdadm --assemble --scan
mdadm: /dev/md/0 assembled from 1 drive - not enough to start the array.
mdadm: No arrays found in config file or automatically

root@raid-test-server:~# ls /sys/class/scsi_host/
host0  host1  host2

root@raid-test-server:~# echo "- - -" > /sys/class/scsi_host/host0/scan
root@raid-test-server:~# echo "- - -" > /sys/class/scsi_host/host1/scan
root@raid-test-server:~# echo "- - -" > /sys/class/scsi_host/host2/scan

root@raid-test-server:~# lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 19.1G  0 disk  
├─sda1    8:1    0   19G  0 part  /
├─sda14   8:14   0    1M  0 part  
└─sda15   8:15   0  122M  0 part  /boot/efi
sdb       8:16   0   10G  0 disk  
└─sdb1    8:17   0   10G  0 part  
  └─md0   9:0    0   20G  0 raid5 
sdc       8:32   0   10G  0 disk  
└─sdc1    8:33   0   10G  0 part  
  └─md0   9:0    0   20G  0 raid5 
sr0      11:0    1 1024M  0 rom 

root@raid-test-server:~# echo "- - -" > /sys/class/scsi_host/host2/scan

root@raid-test-server:~# lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 19.1G  0 disk  
├─sda1    8:1    0   19G  0 part  /
├─sda14   8:14   0    1M  0 part  
└─sda15   8:15   0  122M  0 part  /boot/efi
sdb       8:16   0   10G  0 disk  
└─sdb1    8:17   0   10G  0 part  
  └─md0   9:0    0   20G  0 raid5 
sdc       8:32   0   10G  0 disk  
└─sdc1    8:33   0   10G  0 part  
  └─md0   9:0    0   20G  0 raid5 
sr0      11:0    1 1024M  0 rom

root@raid-test-server:~# cat /proc/mdstat
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] 
md0 : active (auto-read-only) raid5 sdc1[1] sdb1[0]
      20948992 blocks super 1.2 level 5, 512k chunk, algorithm 2 [3/2] [UU_]
      
unused devices: <none>

root@raid-test-server:~# wipefs /dev/sdc -fa
/dev/sdc: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54
/dev/sdc: 8 bytes were erased at offset 0x27ffffe00 (gpt): 45 46 49 20 50 41 52 54
/dev/sdc: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa


root@raid-test-server:~# mdadm --manage --stop /dev/md/0
mdadm: stopped /dev/md/0

root@raid-test-server:~# mdadm --assemble --scan
mdadm: /dev/md/0 has been started with 2 drives (out of 3).

root@raid-test-server:~# wipefs /dev/sdc1 -fa
/dev/sdc1: 4 bytes were erased at offset 0x00001000 (linux_raid_member): fc 4e 2b a9

root@raid-test-server:~# cat /proc/mdstat
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] 
md0 : active (auto-read-only) raid5 sdb1[0] sdc1[1]
      20948992 blocks super 1.2 level 5, 512k chunk, algorithm 2 [3/2] [UU_]
      
unused devices: <none>

root@raid-test-server:~# mdadm --manage --stop /dev/md/0
mdadm: stopped /dev/md/0

root@raid-test-server:~# mdadm --assemble --scan
mdadm: /dev/md/0 assembled from 1 drive - not enough to start the array.
mdadm: No arrays found in config file or automatically

root@raid-test-server:~# cat /proc/mdstat
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] 
md0 : inactive sdb1[0](S)
      10474496 blocks super 1.2
       
unused devices: <none>

root@raid-test-server:~# mdadm --manage --stop /dev/md/0
mdadm: stopped /dev/md/0

```

It is easy to see, that actually a lot of effort is required to destroy a healthy mdadm RAID array. To continue using the remaining drive we still have to stop the RAID at the end, even though the RAID is inactive.

As you saw, you need to run `wipefs /dev/sdc1 -fa` to make it impossible for mdadm to re-assemble the RAID. The conclusion is: RAID 5 can be insecure. During a rebuild a second drive might fail, especially due to the increased load on the disks. This would lead to permanent data loss, even if only a few bytes where destroyed as with the mentioned command. If you need high reliability, you should consider using at least a RAID 6.

<br>

**To make the RAID persistent, please review section 2.2**

<br>

#### Situation 2: Tuning the used partition area to be prepared for drive size variation, understanding the math and units

>**Hint**:
>In this example 8000MB sectors at the end of the drive are deliberately left unused to be prepared for failures. It allows you to use drives that have a few sectors less as replacements due to the space left free. The tool parted supports the combination of percentage and MB (=default if nothing was specified). Please do not confuse MB and MiB.

- 1000MB = 1GB
- 1024MiB = 1GiB

So using `-8192` with parted actually would create a warning about bad alignment.

```bash
root@raid-test-server:~# parted --script /dev/sdc mklabel gpt && parted -a optimal -- /dev/sdc mkpart primary 0% -8192
Warning: The resulting partition is not properly aligned for best performance: 34s % 2048s != 0s
Ignore/Cancel? cancel
```

Instead we will use the correct size for good alignment in GB

```bash
parted -a optimal -- /dev/sdb mkpart primary 0% -8000
parted -a optimal -- /dev/sdc mkpart primary 0% -8000
```

You can check the output with `lsblk`

```bash
root@raid-test-server:~# lsblk                                            
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda       8:0    0 19.1G  0 disk 
├─sda1    8:1    0   19G  0 part /
├─sda14   8:14   0    1M  0 part 
└─sda15   8:15   0  122M  0 part /boot/efi
sdb       8:16   0   10G  0 disk 
└─sdb1    8:17   0   10G  0 part 
sdc       8:32   0   10G  0 disk 
└─sdc1    8:33   0  2.5G  0 part 
sr0      11:0    1 1024M  0 rom  
```

As you can see, `sdc1` is smaller. In this case we are using virtual 10GB drives, so there is only 2.5GB of Storage left. The math behind that situation is in this case:

- Parted: 8000MB = 7629.4MiB
- lsblk on sdc1: 2.5GB = 2384.2MiB
- Sum: 7629.4MiB + 2384.2MiB = 10013.6MiB
- Converting to GiB: 10013.6MiB / 1024MiB = 9.8GiB

As you can see, the total storage does not match the 10GiB we had mounted to our cloud server
 before. If you want exact readings in bytes, you can use `lsblk -b`

```
root@raid-test-server:~# lsblk -b | grep sdc
sdc       8:32   0 10737418240  0 disk 
└─sdc1    8:33   0  2736783360  0 part
```

- Conversion: 2736783360 / 1024 / 1024 = 2610MiB (exact)
- Sum: 2610MiB + 7629.4MiB = 10239.4MiB
- Conversion: 10239.4MiB / 1024 = 9.9994 GiB

As you can see, at this point we almost perfectly match the 10GiB, that we mounted in the Hetzner Cloud panel. With this calculation example you should be able to plan ahead exactly, how much space you might want to leave without wasting space and causing bad alignment.

<br>

#### Situation 3: Advanced operations to reduce interaction with the server, understanding your shell better

Sometimes certain tools expect the user to verify an action. This is especially true with parted.

Our goal here is:
- Start from a point, where none of the drives is in use by any other program. If it is part of a RAID array, you can specify the array(s) under `stop_raid="/dev/md0 /dev/md1"&&\`
- Wipe the drives 1 and 2 (`/dev/sdb` and `/dev/sdc`) and the first 3 sub partition if they exist
- create a GPT partition table with one partition of the full size on both drives.
- Create a RAID with the specified name `create_raid="/dev/md1"&&\` of level 1 and `--metadata=1.0` to avoid being asked before creation
- Get a green success message, if the last command creating a RAID succeeded
- Change the console color back to none

**Warning: This will instantly wipe the drives, ensure that you do not write the wrong drive**
Here is a possible solution:

```
drive_1="/dev/sdb"              &&\
drive_2="/dev/sdc"              &&\
partition_style="gpt"           &&\
stop_raid="/dev/md/0 /dev/md/1"   &&\
create_raid="/dev/md/1"          &&\
raid_level="1"                  &&\
echo $'\n\n\n                   \033[0;32m ALL VARIABLES ARE SET\n\n \033[0m' &&\
mdadm --stop $stop_raid ; # semicolon instead && to avoid the code breaking, if RAID cannot be stopped and error gets returned\
wipefs -fa ${drive_1}1 ${drive_1}2 ${drive_1}3 ; # Wiping existing sub-partition, if it exists, continue if it fails (semicolon)\
wipefs -fa ${drive_2}1 ${drive_2}2 ${drive_2}3 ; # Wiping existing sub-partition, if it exists, continue if it fails (semicolon)\
wipefs -fa $drive_1 $drive_2 &&\
parted --script $drive_1 mklabel $partition_style &&\
parted --script -a optimal -- $drive_1 mkpart primary 0% 100% &&\
parted --script $drive_2 mklabel $partition_style &&\
parted --script -a optimal -- $drive_2 mkpart primary 0% 100% &&\
sleep 1 && # We need the second of wait, or mdadm might not find the drives and gives an error\
mdadm --stop $stop_raid ; # Running again in case mdadm built ghost array using existing partitions(can happen)
mdadm --create $create_raid --metadata=1.2 --force --level=$raid_level --raid-devices=2 ${drive_1}1 ${drive_2}1 &&\
echo $'\n\n\n     \033[0;32m >>> Successfully performed all actions <<<\n\n \033[0m'
```

What can be learned from this example?

- You can combine multiple commands into one to save time
- You can use `\` on the end of each line, to write the commands over multiple lines
- You can use `;` to connect multiple commands and continue, if one fails `dd wrong parameter ; echo this message still prints!`
- You can use `&&` to connect multiple commands and stop if one fails `dd wrong parameter && echo but this message will not print!`
- You can even use `&` to run multiple commands in threaded workload `apt install docker.io -y & htop # Install docker.io and watch monitor server usage, very buggy combination`
- You can set variables and later read them, if they are required multiple times either:
  - standalone`$drive_1` to return `/dev/sdb` or
  - combined without spaces `drive=${drive_1}1` to return `drive=/dev/sdb1`
- Some commands take multiple identical arguments like mdadm `--stop /dev/md0 /dev/md1` and run the action on all
- You can use echo with color and formatting to debug long code using `echo $'          Line 1 a bit on the right\nLine 2 on the left'`
- You can even use comments to explain the functionality using `echo "This is a message"    # We are using echo to print a message\`
- Writing this might take a while, but if you need to run it multiple times and do not use automation, this might save a lot of time
- If you need to do this on multiple servers and correctly wrote your commands, you would get the same result on all machine, because you do not forget a command

<br>

#### Situation 4: RAID 5 rebuild, if your RAID does treat the re-added disk as spare, if the wrong disk got replaced by accident.

If you had a disk failure and removed the wrong disk from your RAID 5 your RAID will fail. The first step is, to add the hard drive back to the server. In this theoretical situation mdadm understands the disk as a spare drive and we are looking for a possible situation to recover data from it. This can work, but if your RAID has further issues this might not work and will definitely result in a full data loss. So it would be best to mirror the contents of each drive anyways.

In this situation our defective drive was /dev/sdc but /dev/sdb got pulled out of the array. We will stop the RAID and manually recreate it with /dev/sdb and force mdadm to assume a clean RAID, which is degraded.

```bash
root@raid-test-server:~# mdadm --stop /dev/md/0

root@raid-test-server:~# mdadm --create /dev/md/0 --level=5 --raid-devices=4 --chunk=64 --name=RackStation:2 /dev/sdb1 missing /dev/sdd1 /dev/sde1 --assume-clean
```

After that we can add /dev/sdc1 to the RAID. This drive was the defective drive, which by now should have been replaced and you should have cloned the partition table to it as listed under section 4.2. This should start the recovery process:

```bash
root@raid-test-server:~# mdadm --add /dev/md/0 /dev/sdc1

root@raid-test-server:~# cat /proc/mdstat
md0 : active raid5 sdc1[4] sdd1[3] sdb1[1] sde1[0]
      2916120768 blocks super 1.2 level 5, 64k chunk, algorithm 2 [4/3] [UU_U]
      [>....................]  recovery =  0.0% (143360/972040256) finish=338.9min speed=47786K/sec
```

If this operation fails, your data are gone. In that case you should run a long SMART check on the existing drives and if all drives are healthy, you can now create a new RAID and deploy your last backup on this. RAID 6 would also have reduced the risk of this data loss.

<br>

<br>

## Conclusion

In this article we reported how to select a suitable RAID level for your project and then configure it on Linux systems using `mdadm`.
Furthermore, administrative tasks like expanding a RAID or exchanging defective hard disks will be discussed.

##### License: MIT

<!---

Contributors's Certificate of Origin

By making a contribution to this project, I certify that:

(a) The contribution was created in whole or in part by me and I have
    the right to submit it under the license indicated in the file; or

(b) The contribution is based upon previous work that, to the best of my
    knowledge, is covered under an appropriate license and I have the
    right under that license to submit that work with modifications,
    whether created in whole or in part by me, under the same license
    (unless I am permitted to submit under a different license), as
    indicated in the file; or

(c) The contribution was provided directly to me by some other person
    who certified (a), (b) or (c) and I have not modified it.

(d) I understand and agree that this project and the contribution are
    public and that a record of the contribution (including all personal
    information I submit with it, including my sign-off) is maintained
    indefinitely and may be redistributed consistent with this project
    or the license(s) involved.

Signed-off-by: markus@omg-network.de

-->
