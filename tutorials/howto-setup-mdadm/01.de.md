---
SPDX-License-Identifier: MIT
path: "/tutorials/howto-setup-mdadm/de"
slug: "howto-setup-mdadm"
date: "2019-06-18"
title: "Software RAID unter Linux"
short_description: "Installation und Konfiguration eines Software RAIDs (mdadm) auf Linuxsystemen."
tags: ["Linux", "RAID", "mdadm"]
author: "Markus"
author_link: "https://github.com/BackInBash"
author_img: "https://avatars3.githubusercontent.com/u/48181660"
author_description: ""
language: "de"
available_languages: ["de", "en"]
header_img: "header-8"
cta: "dedicated"
---

## Einleitung

Hier geht es um die Installation, Einrichtung und anschließende Administration eines `mdadm` Software RAIDs auf Linuxsystemen.

**Voraussetzungen**

+ Ein installiertes Linux OS
+ Einen Dedicated Server mit mindesten 2 freien Partitionen auf 2 verschiedenen Festplatten
+ SSH Root Zugriff

**Kurze Erklärungen**

- In diesem Tutorial wird `/dev/md/0` verwendet, wodurch in den meisten Fällen `/dev/md/0` und `/dev/md0` genutzt werden können. Es ist auch möglich stattdessen `/dev/md0` zu verwenden. Wenn es in Skripten oder Programmen verwendet wird, kann in diesem Fall aber `/dev/md/0` nicht mehr genutzt werden. Wenn bereits ein RAID namens `/dev/md0` erstellt wurde, muss das weiterhin verwendet werden anstelle von `/dev/md/0`.
- `/dev/md/0` muss mit dem eigenen RAID-Namen ersetzt werden, wie beispielsweise `/dev/md127`.
- Zusätzlich werden folgende Beispiel-Festplatten verwendet: `/dev/sdb` - `/dev/sde` >>> Diese müssen in den Beispielbefehlen mit den Namen der eigenen Festplatten ersetzt werden.
- Folgende Beispiel-Partitionen werden verwendet: `/dev/sdb1` - `/dev/sde1` >>> Diese müssen in allen Beispielbefehlen mit den Namen der eigenen Partitionen ersetzt werden.
- Selbst wenn nur ein Schritt ausgeführt wird, sollte alles im Kontext durchgelesen werden, da sonst wichtige Informationen verloren gehen könnten.
- In diesem Tutorial wird auf `/etc/mdadm/mdadm.conf` verwiesen. In manchen Systemen könnte sich diese Config aber auch unter `/etc/mdadm.conf` befinden. In diesem Fall muss entsprechend der korrekte Pfad angegeben werden.
- Für mdadm gibt es sehr viele Parameter. Diese können nicht alle hier gelistet werden. Um Parameter zu nutzen, die in diesem Tutorial nicht verwendet und erklärt wurden, kann die offizielle Dokumentation zu mdadm verwendet werden. Dort werden alle Parameter detailliert erklärt.

## Schritt 1 - Vorbereitungen

Als erstes sollte man sich Gedanken darüber machen, welches RAID-System man betreiben möchte. Dies ist zum einen davon abhängig, welches Ziel man verfolgt und zum anderen wie viele Festplatten im Server selbst verbaut sind.

>
> ***Hinweis:*** Ein RAID sollte nicht als Datensicherung gesehen werden, da es keinen Schutz vor Datenverlust bietet, sondern nur die Verfügbarkeit der Daten erhöht. Kapazität und Leistung richten sich immer nach der Festplatte mit den schlechtesten Werten.
>

### Schritt 1.1 - Auswahl des RAID-Levels

Die Auswahl des richtigen RAID-Levels ist nicht ganz einfach und hängt von mehreren Faktoren ab:
+ Wie viele Festplatten bietet der Server?
+ Welche Ziele verfolgt man?
    + Mehr Speicherplatz / Geringere Verfügbarkeit
    + Höhere Verfügbarkeit / Weniger Speicherplatz
    + Ein Kompromiss zwischen beiden

<details>

<summary>Auflistung der meistverwendeten RAID-Level ausklappen</summary>

<br>

**RAID0**:

Ist ein Verbund aus zwei oder mehr Partitionen. Dabei werden die Partitionen logisch zu einer Partition vereint.
Hier findet eine Erniedrigung der Verfügbarkeit statt. Ist eine der Festplatten defekt sind ***automatisch alle Daten verloren***.

[Weitere Informationen zu RAID0](https://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_1)

+ Pro
    + Erhöht den Verfügbaren Speicherplatz
    + Erhöht die Festplatten Performance
+ Contra
    + Bei einem Festplattenausfall sind die Daten aller Festplatten verloren

---------

**RAID1**:

Ist ein Verbund aus zwei oder mehr Partitionen. Dabei befinden sich die Daten jeweils gespiegelt auf den beiden Partitionen.

[Weitere Informationen zu RAID1](https://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_1)

+ Pro
    + Erhöht die Ausfallsicherheit / Verfügbarkeit der Daten
    + Erhöht die Lesegeschwindigkeit der Daten
    + Keine zusätzliche CPU-Belastung
+ Contra
    + Der verfügbare Speicherplatz halbiert sich
    + Die Lesegeschwindigkeit ist gleich oder schlechter, wenn eine der Festplatten schlechter ist

---------

**RAID5**:

Ist ein Verbund aus drei oder mehr Festplatten, wobei auf einer der Festplatten so genannte "Paritäten" gespeichert werden (Redundanz).

[Weitere Informationen zu RAID5](https://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_5)

+ Pro
    + Erhöhte Ausfallsicherheit / Verfügbarkeit der Daten.
    + Optimale Speichernutzung
    + Erhöht die Lesegeschwindigkeit der Daten
+ Contra
    + Weniger Performance bei Schreibzugriffen
    + Erhöhte CPU-Last

Beispiele:

- 3 Festplatten á 1TB = 2TB (33% Kapazitätsverlust)
- 4 Festplatten á 1TB = 3TB (25% Kapazitätsverlust)
- 5 Festplatten á 1TB = 4TB (20% Kapazitätsverlust)

---------

**RAID6**:

Ist ein Verbund aus vier oder mehr Festplatten, wobei auf zwei der Festplatten so genannte "Paritäten" gespeichert werden (Redundanz).

The capacity is: (Drive_Count - 2)*DriveCapacity

[Weitere Informationen zu RAID6](https://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_6)

+ Pro
    + Noch mehr Ausfallsicherheit / Verfügbarkeit der Daten.
    + Optimale Speichernutzung
    + Erhöht die Lesegeschwindigkeit der Daten
+ Contra
    + Weniger Performance bei Schreibzugriffen
    + Noch höhere CPU-Last

Beispiele:

- 4 Festplatten á 1TB = 2TB (50% Kapazitätsverlust)
- 5 Festplatten á 1TB = 3TB (40% Kapazitätsverlust)
- 6 Festplatten á 1TB = 4TB (33% Kapazitätsverlust)

---------

**Hybrid-RAID / Multi-Layer-Raid**
Die oben gelisteten RAID-Level werden oft so kombiniert:

- RAID 1+0 - (Extreme Performance+Redundanz)
- RAID 0+1 - (Extreme Performance+Redundanz)
- RAID 5/6+0 - (Hohe Redundanz+Performance)
- RAID 0+5/6 - (Hohe Redundanz+Performance)
- RAID 5/6+1 - (Sehr Hohe Redundanz)
- RAID 1+5/6 - (Sehr Hohe Redundanz)

</details>

### Schritt 1.2 - RAID-Setup vorbereiten: Auflistung der Festplatten im System

Für eine kurze und übersichtliche Liste aller verfügbaren Block Devices kann der Befehl `lsblk` verwendet werden.
Hier ein Beispiel-Output von einem Cloud Server mit 2 angehängten Festplatten:

```bash
root@raid-test-server:~# lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda       8:0    0 19.1G  0 disk 
├─sda1    8:1    0   19G  0 part /
├─sda14   8:14   0    1M  0 part 
└─sda15   8:15   0  122M  0 part /boot/efi
sdb       8:16   0   10G  0 disk 
sdc       8:32   0   10G  0 disk 
```

Die Festplatten `sdb` und `sdc` besitzen bisher noch keinen Mountpoint und keine Partitionen.

Für eine Liste mit genaueren Informationen der Partitionen kann `fdisk -l` verwendet werden.

<details>

<summary>Hier klicken für einen Beispiel-Output von <kbd>fdisk -l</kbd></summary>

```bash
root@raid-test-server:~# fdisk -l


Disk /dev/sdc: 10 GiB, 10737418240 bytes, 20971520 sectors
Disk model: Volume          
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes


Disk /dev/sda: 19.07 GiB, 20480786432 bytes, 40001536 sectors
Disk model: QEMU HARDDISK   
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: gpt
Disk identifier: 1B082DDE-A26B-46D7-9223-F1BABA7BBD2D

Device      Start      End  Sectors  Size Type
/dev/sda1  253952 40001502 39747551   19G Linux filesystem
/dev/sda14   2048     4095     2048    1M BIOS boot
/dev/sda15   4096   253951   249856  122M EFI System

Partition table entries are not in disk order.


Disk /dev/sdb: 10 GiB, 10737418240 bytes, 20971520 sectors
Disk model: Volume          
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
```

</details>

>
> **Hinweis**:
> Für ein Software RAID, muss nicht die gesamte Festplatte dem RAID hinzugefügt werden. Es reichen einzelne Partitionen.
>

## Schritt 2 - Erstellen eines Software RAIDs

### Schritt 2.1 - Vorbereiten der Festplatten

Zunächst müssen die Festplatten entsprechend formatiert werden.

> 
> Wenn die untenstehenden Befehle nicht funktionieren, da "parted" nicht gefunden werden konnte, kann es so installiert werden:
> ```bash
> apt update && apt install parted -y`
> ```
> 

Eine neue, leere Partitionstabelle auf dem Laufwerk erstellen:

+ Für Festplatten größer als 2 TB oder PCs mit UEFI:
  ```bash
  parted /dev/sdb mklabel gpt
  parted /dev/sdc mklabel gpt
  ```

+ Für Festplatten kleiner als 2 TB und BIOS:
  ```bash
  parted /dev/sdb mklabel msdos
  parted /dev/sdc mklabel msdos
  ```

Partition auf der Festplatte anlegen:
```bash
parted -a optimal -- /dev/sdb mkpart primary 0% 100%
parted -a optimal -- /dev/sdc mkpart primary 0% 100%
```

Auf alten Systemen muss die neuerstellte Partition gegebenenfalls als RAID-Partition markiert werden:

```bash
parted /dev/sdb set 1 raid on
parted /dev/sdc set 1 raid on
```

### Schritt 2.2 - Anlegen des Software RAIDs

Unter Linux ist `mdadm` das Hauptwerkzeug. Es bildet die Schnittstelle zu den RAID-Funktionen des Kernels.

- **Raid 1**
  
  Um ein RAID 1 anzulegen genügt der folgende Befehl:
  
  ```bash
  mdadm --create /dev/md/0 --auto md --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1
  ```

- **Raid 5**
  
  Wenn es noch eine dritte Festplatte gibt, kann man ein RAID 5 erstellen. Der Schritt 2.1 muss dafür zusätzlich auch für die dritte Festplatte ausgeführt werden und für alle anderen Festplatten, die hinzugefügt werden. Wenn die dritte Festplatte `/dev/sdd1` ist, genügt der folgende Befehl, um ein RAID 5 anzulegen:
  
  ```bash
  sudo mdadm --create /dev/md0 --auto md --level=5 --raid-devices=4 /dev/sda1 /dev/sdb1 /dev/sdc1 /dev/sdd1
  ```

- **Raid 6**
  
  Wenn es noch eine vierte Festplatte gibt, kann man ein RAID 6 erstellen. Dafür kann man folgenden Befehl nutzen:
  
  ```bash
  mdadm --create /dev/md/0 --auto md --level=6 --raid-devices=4 /dev/sdb1 /dev/sdc1 /dev/sdd1 /dev/sde1
  ```

<br>

Das neu erstellte Block Device `mdX` kann sofort benutzt werden und das System darf auch währenddessen heruntergefahren oder neu gestartet werden.
Während Sync oder Rebuild kann die Performance merkbar abnehmen.
Der aktuelle Status der RAID-Erstellung kann mit folgendem Befehl abgefragt werden:

```bash
watch cat /proc/mdstat
```

Während dem Erstellungsprozess wird angezeigt:

```bash
Personalities : [raid0] [raid1]
mdX : active raid1 sdb1[1] sdc1[0]
      8384448 blocks super 1.2 [2/2] [UU]
      [==============>.......] check = 74.7% (6267520/8384448) finish=0.1min speed=202178K/sec
```

Formatieren des neuerstellten RAIDs, hier mit ext4:

```bash
mkfs.ext4 /dev/md/0
```

Um den RAID im System nutzen zu können, muss ein Ordner erstellt werden, auf den der RAID eingehängt werden kann. In diesem Fall wird `/dev/md/0` auf einen Ordner namens `/mnt/md0-mount` eingehängt.

```bash
mkdir /mnt/md0-mount
mount /dev/md/0 /mnt/md0-mount
```

<br>

**Automatisches Erstellen des RAID**

Damit der RAID auch nach einem Reboot korrekt erstellt wird, müssen folgende Befehle ausgeführt werden:

```bash
mdadm --detail --scan >> /etc/mdadm/mdadm.conf
update-initramfs -u
```

Wenn man den ersten Befehl ausführt, ohne diesen in der `mdadm.conf`-Datei hinzuzufügen, werden die Informationen angezeigt, die mdadm benötigt, um den RAID nach einem Reboot zu starten:

```bash
root@raid-test-server:~# mdadm --detail --scan
ARRAY /dev/md/0 metadata=1.2 name=raid-test-server:1 UUID=e2d1049e:d6c61312:5a8d0975:3fac53c0
```

Nach einem Reboot kann der RAID gegebenenfalls einen anderen Namen haben:

```bash
root@raid-test-server:~# lsblk
NAME      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sda         8:0    0 19.1G  0 disk  
├─sda1      8:1    0   19G  0 part  /
├─sda14     8:14   0    1M  0 part  
└─sda15     8:15   0  122M  0 part  /boot/efi
sdb         8:16   0   10G  0 disk  
└─sdb1      8:17   0   10G  0 part  
  └─md127   9:127  0   10G  0 raid1 
sdc         8:32   0   10G  0 disk  
└─sdc1      8:33   0   10G  0 part  
  └─md127   9:127  0   10G  0 raid1 
```

<br>

**Automatisches Einhängen des RAIDs**

Nach einem Reboot ist RAID nicht mehr eingehängt. Damit RAID nach einem Reboot automatisch eingehängt wird, sind folgende Schritte nötig:

- Die UUID des RAIDs herausfinden, als zuverlässige Referenz (funktioniert auch, wenn `md0` in `md1` umbenannt wird)
- Einen neuen Eintrag in der `/etc/fstab`-Datei ergänzen

```bash
root@raid-test-server:~# blkid | grep md0
/dev/md0: UUID="2e641692-d8c3-4b4c-ad08-923af60b6e49" BLOCK_SIZE="4096" TYPE="ext4"
```

In diesem Beispiel ist die UUID: `2e641692-d8c3-4b4c-ad08-923af60b6e49`

Die UUID — **und nur die UUID** — muss im ersten Befehl mit der eigenen UUID ersetzt werden. Dann können die beiden Befehle ausgeführt werden:

```bash
root@raid-test-server:~# echo "UUID=2e641692-d8c3-4b4c-ad08-923af60b6e49 /mnt/md0-mount ext4 defaults 0 2" >> /etc/fstab
root@raid-test-server:~# echo $'\n\033[0;32m     Entries in your /etc/fstab: \033[0;34m';cat /etc/fstab | grep . | egrep -v '#.*';echo $'\n\033[0m'

     Entries in your /etc/fstab: 
UUID=c8d00ef8-b9d3-4c55-8007-3dec27a2891a /               ext4    errors=remount-ro 0       1
UUID=3612-8A38  /boot/efi       vfat    umask=0077      0       1
/dev/sr0        /media/cdrom0   udf,iso9660 user,noauto     0       0
UUID=2e641692-d8c3-4b4c-ad08-923af60b6e49 /mnt/md0-mount ext4 defaults 0 2
```

In der letzten Zeile sollte wie in dem Beispiel "**UUID=your_uuid /mount/folder xt4 defaults 0 2**" stehen. Nur wenn es dort steht, kann ein Reboot ausgeführt werden, um zu testen ob das automatische Einhängen funktioniert. Das kann mit dem Befehl `lsblk` geprüft werden. Falls nicht, kann mit nano oder einem beliebigen anderen Editor die Zeile entfernt werden. Es sollte geprüft werden, ob die UUID korrekt ist und ob sich ein Tippfehler eingeschlichen hat.

Falls etwas nicht funktioniert, kann das [Rescue-System](https://docs.hetzner.com/de/cloud/servers/getting-started/rescue-system/) genutzt werden, um folgende Befehle auszuführen:

```bash
mount /dev/sda1 /mnt
nano /mnt/etc/fstab
```

Angenommen das OS liegt auf `/dev/sda1`, dann kann fstab bearbeitet und die letzte Zeile entfernt werden. Nachdem die Änderungen mit `CTRL`+`S` gespeichert und die Datei mit `CTRL`+`X` geschlossen wurde, kann der Server neugestartet werden.

### Schritt 2.3 - Hotspare hinzufügen

Wenn man seinen RAID schützen möchte, kann man ein Hotspare hinzufügen. Sollte eine der Festplatten ausfallen und der RAID den Status "degraded" besitzen, wird die Hotspare-Festplatte automatisch dem RAID hinzugefügt und als Ersatz genutzt.

In diesem Beispiel wird angenommen, dass `/dev/sdd` die dritte Festplatte für RAID 1 ist, welcher zwei Festplatten benötigt. Auf der neuen Festplatte wurden auch bereits alle Schritte von "Schritt 2.1" ausgeführt.

```bash
mdadm --manage /dev/md/0 --add-spare /dev/sdd1
```

Folgender Befehl zeigt den aktuellen Status:

```bash
root@raid-test-server:~# mdadm --detail /dev/md/0 | egrep '(Devices|State|Number|dev|Size)'
/dev/md/0:
        Array Size : 10474496 (9.99 GiB 10.73 GB)
     Used Dev Size : 10474496 (9.99 GiB 10.73 GB)
      Raid Devices : 2
     Total Devices : 3
             State : clean 
    Active Devices : 2
   Working Devices : 3
    Failed Devices : 0
     Spare Devices : 1
    Number   Major   Minor   RaidDevice State
       0       8       33        0      active sync   /dev/sdc1
       1       8       17        1      active sync   /dev/sdb1
       2       8       49        -      spare   /dev/sdd1

```

<br>

**Hotspare testen**

Hier wird `/dev/sdb` über die [Cloud Console](https://console.hetzner.cloud/) vom Cloud Server entfernt. Das sieht dann so aus:

```bash
root@raid-test-server:~# cat /proc/mdstat
Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sdd1[2](S) sdb1[1] sdc1[0]
      10474496 blocks super 1.2 [2/2] [UU]
      
unused devices: <none>
```
```bash
root@raid-test-server:~# lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sda       8:0    0 19.1G  0 disk  
├─sda1    8:1    0   19G  0 part  /
├─sda14   8:14   0    1M  0 part  
└─sda15   8:15   0  122M  0 part  /boot/efi
sdc       8:32   0   10G  0 disk  
└─sdc1    8:33   0   10G  0 part  
  └─md0   9:0    0   10G  0 raid1 /mnt/md0-mount
sdd       8:48   0   10G  0 disk  
└─sdd1    8:49   0   10G  0 part  
  └─md0   9:0    0   10G  0 raid1 /mnt/md0-mount
sr0      11:0    1 1024M  0 rom   
```
```bash
root@raid-test-server:~# cat /proc/mdstat
Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sdd1[2] sdc1[0]
      10474496 blocks super 1.2 [2/1] [U_]
      [==>..................]  recovery = 11.7% (1229632/10474496) finish=1.1min speed=136625K/sec
      
unused devices: <none>

#### Waiting a few seconds till the 10GB RAID is rebuilt ####
```
```bash
root@raid-test-server:~# cat /proc/mdstat
Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sdd1[2] sdc1[0]
      10474496 blocks super 1.2 [2/2] [UU]
```

Erst sind alle 3 Festplatten sichtbar, ein paar Sekunden später zeigt lsblk an, dass `/dev/sdb` nicht existiert. Der Output von `cat /proc/mdstat` zeigt, dass der RAID bereits aktualisiert wird. Diese Automation kann deutlich Zeit sparen, es kann aber auch zu Schwierigkeiten führen. Wenn der Status der Festplatten nicht gut überwacht wird, und eine Festplatte nicht mehr genutzt werden kann, könnte es passieren, dass das Syncen fehlschlägt. Wenn nur ein paar Kilobytes an Daten wichtig sind und der Rest kann schnell wieder hergestellt werden, ist es vermutlich besser von diesen paar Dateien ein Backup zu erstellen, bevor der RAID aktualisiert wird.

Wenn `/dev/sdb` dem Server wieder hinzugefügt wird, wird es dem RAID nicht automatisch als Hotspare hinzugefügt. Das kann man aber manuell machen:

```bash
root@raid-test-server:~# mdadm --manage /dev/md/0 --add-spare /dev/sdb1
mdadm: added /dev/sdb1

root@raid-test-server:~# cat /proc/mdstat
Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sdb1[3](S) sdd1[2] sdc1[0]
      10474496 blocks super 1.2 [2/2] [UU]
      
unused devices: <none>
```

Als nächsten Test kann `/dev/sdd` entfernt werden, diesmal über den offiziellen Weg mit `--fail` und `--remove`.

```bash
root@raid-test-server:~# mdadm --manage /dev/md/0 --fail /dev/sdd1
mdadm: set /dev/sdd1 faulty in /dev/md/0

root@raid-test-server:~# mdadm --manage /dev/md/0 --remove /dev/sdd1
mdadm: hot removed /dev/sdd1 from /dev/md/0

root@raid-test-server:~# cat /proc/mdstat 
Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sdb1[2] sdc1[0]
      10474496 blocks super 1.2 [2/1] [U_]
      [=>...................]  recovery =  8.4% (888320/10474496) finish=0.8min speed=177664K/sec
      
unused devices: <none>
```

## Schritt 3 - Auflösen eines Software RAIDs

Um ein Software RAID aufzulösen, in diesem Fall RAID1 ohne Hotspare, müssen folgende Schritte ausgeführt werden:

1. **RAID stoppen**
   
   ```bash
   umount /dev/md/0
   mdadm --stop /dev/md/0
   ```
1. **Automatische Mount-Einträge entfernen** (z.B. `/etc/fstab`)
   
   ```bash
   nano /etc/fstab
   
   # Jetzt den Eintrag von zuvor entfernen:
   # UUID=2e641692-d8c3-4b4c-ad08-923af60b6e49 /mnt/md0-mount ext4 defaults 0 2
   # initramfs mit diesem Befehl aktualisieren:
   
   update-initramfs -u
   ```

1. **RAID-Einträge in `/etc/mdadm/mdadm.conf` löschen**
   
   ```bash
   ARRAY /dev/md/0 metadata=1.2 name=raid-test-server:1 UUID=e2d1049e:d6c61312:5a8d0975:3fac53c0
   ```

1. **Den Superblock der verwendeten Partitionen löschen**
   
   ```bash
   mdadm --zero-superblock /dev/sdb1 /dev/sdc1
   ```

1. **RAID flag deaktivieren**
   
   ```bash
   parted /dev/sdb set 1 raid off
   parted /dev/sdc set 1 raid off
   ```

## Schritt 4 - Verwaltung eines Software RAIDs

### Schritt 4.1 - RAID-Status ermitteln

Eine kurze Auflistung aller RAIDs im System erhält man mit der Ausgabe der Datei `/proc/mdstat`:

```bash
root@raid-test-server:~# cat /proc/mdstat
```

<details>

<summary>Beispiel-Output:</summary>

```bash
Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sdb1[3](S) sdd1[2] sdc1[0]
      10474496 blocks super 1.2 [2/2] [UU]

md2 : active raid1 sdb3[1] sdc3[0]
      536739840 blocks super 1.2 [2/2] [UU]
      bitmap: 3/4 pages [12KB], 65536KB chunk

md1 : active raid1 sdb2[1] sdc2[0]
      1047552 blocks super 1.2 [2/2] [UU]

unused devices: <none>
```

</details>

Eine genauere Ausgabe zu einem bestimmten RAID bekommt man mit dem Befehl `mdadm --detail /dev/md/0`. Wenn man nur bestimmte Informationen benötigt, kann der Output mittels `grep` gefiltert werden.

```bash
root@raid-test-server:~# mdadm --detail /dev/md/0 | egrep '(Devices|State|Number|dev|Size)'
```

<details>

<summary>Beispiel-Output:</summary>

```bash
/dev/md/0:
        Array Size : 10474496 (9.99 GiB 10.73 GB)
     Used Dev Size : 10474496 (9.99 GiB 10.73 GB)
      Raid Devices : 2
     Total Devices : 3
             State : clean 
    Active Devices : 2
   Working Devices : 3
    Failed Devices : 0
     Spare Devices : 1
    Number   Major   Minor   RaidDevice State
       0       8       33        0      active sync   /dev/sdc1
       1       8       17        1      active sync   /dev/sdb1
       2       8       49        -      spare   /dev/sdd1
```

</details>

### Schritt 4.2 - Defekte Festplatte tauschen

Dazu muss die defekte Festplatte auf "failed" gesetzt und anschließend aus dem RAID entfernt werden:

```bash
root@raid-test-server:~# mdadm --manage /dev/md/0 --fail /dev/sdb1
mdadm: set /dev/sdb1 faulty in /dev/md/0

root@raid-test-server:~# mdadm --manage /dev/md/0 --remove /dev/sdb1
mdadm: hot removed /dev/sdb1 from /dev/md/0
```

Eine Festplatte sollte im besten Fall niemals ersetzt werden, bevor sie vom RAID entfernt wurde. Dadurch können künftige Probleme vermieden werden.

Wenn eine Festplatte ersetzt werden muss, muss die neue Festplatte partitioniert werden. Dabei ist es wichtig, dass die neue Festplatte genauso partitioniert wird, wie es bei der defekten Festplatte der Fall war. Zudem muss die neue Festplatte mindestens so groß sei wie die kleinste Festplatte im RAID. Bei RAID 6 kann theoretisch eine Kombination von 1x 512GB, 2x 6TB, und 1x 12TB Festplatten genutzt werden. Die Kapazität wäre begrenzt auf:

```
lowest_capacity * (drive_count - 2)
= 512GB * (4 -2) = 1TB
```

Wenn die neue Festplatte nicht automatisch erkannt wird, kann der Befehl `partprobe` genutzt werden. Anschließend sollte die Festplatte über `lsblk` angezeigt werden. 

Die neue Festplatte muss partitioniert werden. Es genügt die Partitionstabelle von einer bestehenden, funktionierenden Festplatte zu kopieren. Hier ist `/dev/sdc` zum Beispiel noch im RAID und `/dev/sdb` wurde entfernt und ersetzt und soll jetzt wieder dem RAID hinzugefügt werden.

```bash
# Copying from /dev/sdc because it is healthy
sfdisk -d /dev/sdc > part_table
```

Mit denselben Festplatten und Partitions-IDs wiederherstellen:

```bash
# Pasting into /dev/sdb, which was replaced
sfdisk /dev/sdb < part_table

# Direct copy from /dev/sdc to /dev/sdd in this case
sfdisk -d /dev/sdc | sfdisk /dev/sdd
```

Wenn die Festplatte richtig partitioniert wurde, kann sie wieder dem RAID-System hinzugefügt werden (mdadm führt intern `--re-add` aus, um zu prüfen ob die Festplatte schon mal teil des RAIDs war):

```bash
mdadm --manage /dev/md/0 --add /dev/sdd
```

Wenn die Festplatte als Hotspare genutzt werden soll, kann sie dem RAID-System so hinzugefügt werden:

```bash
mdadm --manage /dev/md/0 --add-spare /dev/sdd
```

Wenn eine Festplatte einem RAID mit zu wenig Festplatten als Hotspare hinzugefügt wird, wird diese Festplatte sofort richtig genutzt.

Der Vorgang kann mit dem Befehl `watch -n 0.5 cat /proc/mdstat` nachverfolgt werden.

>
> **Hinweis**: Sollte das System auf dem RAID selbst liegen, ist es notwendig den Bootloader auf der entsprechenden Festplatte zu installieren. Auf einem laufenden System wird es vermutlich nicht funktionieren, daher ist es besser hierfür das [Rescue-System](https://docs.hetzner.com/de/cloud/servers/getting-started/rescue-system) zu nutzen. Dadurch können Probleme durch Zugriffsrechte vermieden werden. Folgender Befehl kann genutzt werden:
> 
> ```bash
> update grub && grub-install /dev/sda
> ```
>

### Schritt 4.3 - RAID erweitern

Es können nur RAIDs mit Level 1, 4, 5, und 6 erweitert werden.
Wenn ein RAID erweitert wird, muss sowohl das RAID-Level als auch die Größe des Dateisystems angepasst werden.

-------

**RAID-Level anpassen**

<details>

  <summary><b>RAID 1 > RAID 5</b></summary>

  In diesem Beispiel wird ist es wie oben RAID 1 mit `sbd1` und `sbc1`.
  
  - Die zusätzliche Festplatte (`sbd1`) dem RAID hinzufügen:
    
    ```bash
    mdadm /dev/md/0 --add /dev/sdd1
    ```
  
  - RAID-Level anpassen:
    
    ```bash
    root@raid-test-server:~# mdadm --grow --raid-devices=3 --level=5 /dev/md/0 --backup-file=/root/md0.bak
    ```
    
    > **Hinweis**:
    > In der mittels `--backup-file` angegebenen Datei werden kritische Bereiche gesichert (typischerweise einige wenige MiB). Falls das System während der Erweiterung abstürzt, kann die Erweiterung später mittels folgendem Befehl fortgesetzt werden:
    > ```bash
    > mdadm /dev/md/0 --continue --backup-file=/tmp/md0.bak
    > ```
    > Die Sicherungsdatei darf nicht auf dem zu erweiternden RAID liegen! Die Verwendung von `backup-file` ist nicht zwingend notwendig, wird aber dringend empfohlen.

  - Fortschritt anzeigen
    
    ```bash
    root@raid-test-server:~# cat /proc/mdstat 
    Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
    md0 : active raid5 sdb1[2] sdd1[4] sdc1[3]
          10474496 blocks super 1.2 level 5, 64k chunk, algorithm 2 [3/3] [UUU]
          [==>..................]  reshape = 10.5% (1102296/10474496) finish=1.2min speed=122477K/sec
        
    unused devices: <none>
    ```
  
  - Größe des RAIDs anzeigen lassen
    
    ```bash
    root@raid-test-server:~# mdadm --detail /dev/md/0 | grep Size
            Array Size : 20948992 (19.98 GiB 21.45 GB)
         Used Dev Size : 10474496 (9.99 GiB 10.73 GB)
            Chunk Size : 64K
    ```

----------

</details>

<details>

  <summary><b>RAID 5 > RAID 1</b></summary>

  Bei drei Festplatten in einem RAID 5 ist es möglich zurück zu RAID 1 zu wechseln. Das funktioniert aber nur, wenn die zweite Hälfte des RAID-Platzes noch frei ist. Zudem ist es schwierig und gefährlich.
  
  >
  > **Hinweis:** Wenn das Dateisystem bereits erweitert wurde, muss dieses wieder angepasst werden. Das ist ein komplizierter Vorgang, bei welchem Dateien oder Teile von Dateien umgeschrieben werden müssen (auf die erste Hälfte des RAIDs). In diesem Beispiel wurde das Dateisystem noch nicht erweitert, weshalb es nicht verkleinert werden muss und die Daten sind nicht betroffen.
  >
  
  - Versuchen das RAID-Level zu ändern
    
    ```bash
    root@raid-test-server:~# mdadm --grow --raid-devices=2 --level=1 /dev/md/0
    mdadm: Can only convert a 2-device array to RAID1
    ```
  
  - Die Größe verkleinern
    
    ```bash
    root@raid-test-server:~# mdadm --grow --raid-devices=2 --level=5 /dev/md/0
    mdadm: this change will reduce the size of the array.
           use --grow --array-size first to truncate array.
           e.g. mdadm --grow /dev/md/0 --array-size 10474496
    ```
    
    ```bash
    root@raid-test-server:~# mdadm --grow /dev/md/0 --array-size 10474496
    ```

  - RAID ändern
    
    ```bash
    root@raid-test-server:~# mdadm --grow --raid-devices=2 /dev/md/0 --backup-file=/root/md0.bak
    mdadm: Need to backup 128K of critical section..
    ```
  
  - Fortschritt anzeigen
    
    ```bash
    root@raid-test-server:~# cat /proc/mdstat
    Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
    md0 : active raid5 sdb1[2] sdd1[4] sdc1[3]
          10474496 blocks super 1.2 level 5, 64k chunk, algorithm 2 [2/2] [UU]
          [====>................]  reshape = 22.3% (2337732/10474496) finish=1.0min speed=129874K/sec
          
    unused devices: <none>
     
    #### Waiting a few seconds till the 10GB drive RAID is reshaped ####
    ```
  
  - RAID-Level ändern
    
    ```bash
    root@raid-test-server:~# mdadm --grow --raid-devices=2 --level=1 /dev/md/0
    mdadm: level of /dev/md/0 changed to raid1
    ```
  
  - RAID-Details anzeigen
    
    ```bash
    root@raid-test-server:~# mdadm --detail /dev/md/0 | egrep '(Devices|State|Number|dev|Size)'
    /dev/md/0:
            Array Size : 10474496 (9.99 GiB 10.73 GB)
         Used Dev Size : 10474496 (9.99 GiB 10.73 GB)
          Raid Devices : 2
         Total Devices : 3
                 State : clean 
        Active Devices : 2
       Working Devices : 3
        Failed Devices : 0
         Spare Devices : 1
        Number   Major   Minor   RaidDevice State
           4       8       49        0      active sync   /dev/sdd1
           3       8       33        1      active sync   /dev/sdc1
           2       8       17        -      spare   /dev/sdb1
    ```

------

</details>

<details>

  <summary><b>RAID 5 > RAID 5</b></summary>

  Mit RAID5 ist es möglich die Größe des RAIDs anzupassen, ohne das RAID-Level zu ändern. In diesem Beispiel besitzt der RAID5 nur drei Festplatten. Wenn man jetzt angeben würde, dass dieser RAID5 vier Festplatten verwenden muss, würde das zu einem Fehler führen. Daher muss der Befehl mit `--force` ausgeführt werden. Wenn der RAID5 einen Hotspare besitzen würde, der direkt als vierte Festplatte genutzt werden könnte, würde der Befehl auch ohne `--force` funktionieren.
  
  - Beispiel ohne `--force`
    
    ```bash
    root@raid-test-server:~# mdadm --grow --raid-devices=4 --level=5 /dev/md/0 --backup-file=/root/md0.bak
    mdadm: Need 1 spare to avoid degraded array, and only have 0.
           Use --force to over-ride this check.
    ```
  
  - Beispiel mit `--force`
    
    ```bash
    root@raid-test-server:~# mdadm --grow --raid-devices=4 --level=5 /dev/md/0 --backup-file=/root/md0.bak --force
    mdadm: Need to backup 384K of critical section..
    ```
  
  - Fortschritt anzeigen
    
    ```bash
    root@raid-test-server:~# cat /proc/mdstat 
    Personalities : [raid1] [linear] [multipath] [raid0] [raid6] [raid5] [raid4] [raid10] 
    md0 : active raid5 sdb1[2] sdd1[4] sdc1[3]
          10474496 blocks super 1.2 level 5, 64k chunk, algorithm 2 [4/3] [UUU_]
          [=>...................]  reshape =  6.1% (644760/10474496) finish=1.2min speed=128952K/sec
      
    unused devices: <none>
  
    #### Waiting a few seconds till the 10GB drive RAID is reshaped ####
    ```
  
  - RAID-Details anzeigen
    
    ```bash
    root@raid-test-server:~# mdadm --detail /dev/md/0 | egrep '(Devices|State|Number|dev|Size)'
    /dev/md/0:
            Array Size : 10474496 (9.99 GiB 10.73 GB)
         Used Dev Size : 10474496 (9.99 GiB 10.73 GB)
          Raid Devices : 4
         Total Devices : 3
                 State : clean, degraded 
        Active Devices : 3
       Working Devices : 3
        Failed Devices : 0
         Spare Devices : 0
            Chunk Size : 64K
        Number   Major   Minor   RaidDevice State
           4       8       49        0      active sync   /dev/sdd1
           3       8       33        1      active sync   /dev/sdc1
           2       8       17        2      active sync   /dev/sdb1
    ```
  
  - Größe ändern
    
    ```bash
    root@raid-test-server:~# mdadm --grow /dev/md/0 --array-size max
    ```
  
  - Größe des RAIDs anzeigen
    
    ```bash
    root@raid-test-server:~# mdadm --detail /dev/md/0 | grep Size
            Array Size : 31423488 (29.97 GiB 32.18 GB)
         Used Dev Size : 10474496 (9.99 GiB 10.73 GB)
            Chunk Size : 64K
    ```

</details>

-------

**Dateisystem erweitern**

Das Dateisystem muss noch erweitert werden, damit der neu entstandene Speicherplatz genutzt werden kann. Die Erweiterung findet mit folgenden Befehlen statt:

```bash
umount /dev/md/0 /mnt    # Das Dateisystem aushängen
fsck.ext4 -f /dev/md/0   # Die Prüfung erzwingen, selbst wenn vor Kurzem geprüft wurde
resize2fs /dev/md/0      # Das Dateisystem auf Maximalgröße erweitern
mount /dev/md/0 /mnt     # Das Dateisystem wieder einhängen
```

<details>

<summary>Hier klicken für einen Beispiel-Output</summary>

- Das Dateisystem `/dev/md/0` aushängen
  
  ```bash
  root@raid-test-server:~# umount /dev/md/0
  ```

- Die Prüfung erzwingen
  
  ```bash
  root@raid-test-server:~# fsck.ext4 -f /dev/md/0
  e2fsck 1.46.2 (28-Feb-2021)
  /dev/md/0: recovering journal
  Pass 1: Checking inodes, blocks, and sizes
  Pass 2: Checking directory structure
  Pass 3: Checking directory connectivity
  Pass 4: Checking reference counts
  Pass 5: Checking group summary information
  Free blocks count wrong (2551871, counted=2551615).
  Fix<y>? yes
  Free inodes count wrong (655349, counted=655348).
  Fix<y>? yes

  /dev/md/0: ***** FILE SYSTEM WAS MODIFIED *****
  /dev/md/0: 12/655360 files (0.0% non-contiguous), 67009/2618624 blocks
  ```

  >
  > **Hinweis:** In diesem Beispiel wurden zwei Fehler gefunden. Das kommt vermutlich durch mehrmaliges Ändern der Größe. Das Dateisystem sollte nach jeder Änderung geprüft werden.
  >

- Das Dateisystem `/dev/md/0` erweitern
  
  ```bash
  root@raid-test-server:~# resize2fs /dev/md/0
  resize2fs 1.46.2 (28-Feb-2021)
  Resizing the filesystem on /dev/md/0 to 7855872 (4k) blocks.
  The filesystem on /dev/md/0 is now 7855872 (4k) blocks long.
  ```

- Das Dateisystem `/dev/md/0` einhängen

  ```bash       
  root@raid-test-server:~# mount /dev/md/0 /mnt/md0-mount/
  ```

--------

</details>

<br>

Wenn die `/etc/mdadm.conf`-Datei anders ist, muss zum Bearbeiten eventuell dieser Befehl genutzt werden: `nano /etc/mdadm/mdadm.conf`<br>
Dort sollten `num-devices` und `level` angepasst werden.

- ALT :
  
  ```bash
  ARRAY /dev/md/0 level=1 num-devices=2 metadata=1.2 name=raid-test-server:1 UUID=e2d1049e:d6c61312:5a8d0975:3fac53c0
  ```

- NEU:
  
  ```bash
  ARRAY /dev/md/0 level=5 num-devices=3 metadata=1.2 name=raid-test-server:1 UUID=e2d1049e:d6c61312:5a8d0975:3fac53c0
  ```

<br>

In diesem Beispiel enthält die RAID-Config keine wichtigen Informationen, die angepasst werden müssten. Nach einem Reboot wird angezeigt:

```bash
root@raid-test-server:~# mdadm --detail /dev/md/0 | egrep '(Devices|State|Number|dev|Size)' && lsblk | egrep '(─sd.1 |md)'
/dev/md/0:
        Array Size : 31423488 (29.97 GiB 32.18 GB)
     Used Dev Size : 10474496 (9.99 GiB 10.73 GB)
      Raid Devices : 4
     Total Devices : 3
             State : clean, degraded 
    Active Devices : 3
   Working Devices : 3
    Failed Devices : 0
     Spare Devices : 0
        Chunk Size : 64K
    Number   Major   Minor   RaidDevice State
       4       8       33        0      active sync   /dev/sdc1
       3       8       17        1      active sync   /dev/sdb1
       2       8       49        2      active sync   /dev/sdd1
├─sda1    8:1    0   19G  0 part  /
└─sdb1    8:17   0   10G  0 part  
  └─md0   9:0    0   30G  0 raid5 /mnt/md0-mount
└─sdc1    8:33   0   10G  0 part  
  └─md0   9:0    0   30G  0 raid5 /mnt/md0-mount
└─sdd1    8:49   0   10G  0 part  
  └─md0   9:0    0   30G  0 raid5 /mnt/md0-mount
```

In diesem Beispiel wurde RAID korrekt eingerichtet und eingehängt.

>
> **Hinweis**: Durch die Angabe von `mdadm` mit `egrep` und `lsblk` mit `grep` wird nur die Information angezeigt, die relevant ist.
>

### Schritt 4.4 - RAID überwachen

Um das RAID zu überwachen, kann dieser Eintrag als Crontab (`sudo crontab -e`) hinterlegt werden:

```bash
0 0 * * * /usr/share/mdadm/checkarray --cron --all --quiet >/dev/null 2>&1 # Runs every day at 00:00 AM
```

Wenn auf dem Host beispielsweise ein Mailserver installiert ist, kann man sich selbst mittels mdadm E-Mails zusenden lassen, wenn Fehler auftreten. Das kann in /etc/mdadm.conf eingerichtet werden. Die Datei kann mit diesem Befehl bearbeitet werden: `nano /etc/mdadm/mdadm.conf`

- **Situation 1 - Mailserver installiert**
  
  Folgende Zeile muss angepasst werden:
  ```bash
  # instruct the monitoring daemon where to send mail alerts
  MAILADDR your_receiver_mail@mail_provider.com
  ```

- **Situation 2 - msmtp client installiert**
  
  Folgende Zeile muss angepasst werden:
  ```bash
  # instruct the monitoring daemon where to send mail alerts
  MAILADDR your_receiver_mail@mail_provider.com
  MAILFROM your_sender_mail@mail_provider.com
  ```

## Ergebnis

Es wurde erklärt, wie man ein passendes RAID-Level für sein vorhaben auswählt und dieses dann entsprechend auf Linux Systemen mithilfe von `mdadm` konfiguriert. Des weiteren wird auf administrative Tätigkeiten eingegangen, wie zum Beispiel Erweitern eines RAIDs oder das Tauschen defekter Festplatten.

##### License: MIT

<!---

Contributors's Certificate of Origin

By making a contribution to this project, I certify that:

(a) The contribution was created in whole or in part by me and I have
    the right to submit it under the license indicated in the file; or

(b) The contribution is based upon previous work that, to the best of my
    knowledge, is covered under an appropriate license and I have the
    right under that license to submit that work with modifications,
    whether created in whole or in part by me, under the same license
    (unless I am permitted to submit under a different license), as
    indicated in the file; or

(c) The contribution was provided directly to me by some other person
    who certified (a), (b) or (c) and I have not modified it.

(d) I understand and agree that this project and the contribution are
    public and that a record of the contribution (including all personal
    information I submit with it, including my sign-off) is maintained
    indefinitely and may be redistributed consistent with this project
    or the license(s) involved.

Signed-off-by: markus@omg-network.de

-->
